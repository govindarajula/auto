# repo title
* intro to be updated soon
* **Tools used**: (Theano,Lasagne,Nolearn,Amazon EC2)
___
> Network-in-network layers: These are small neural networks that are easier to interpret than tra*ditional neural network layers.

> Dropout layers: These randomly drop units during training, preventing overfitting, which is a major problem in neural networks.

> Noise layers: These introduce noise into the neurons; again, addressing the overfitting problem.
___
* Footnotes: 

 Reference: Learning Data mining with Python by Robert Layton.

[![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/) 
